{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "586134b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Add the current directory to path to import local modules\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from model.architecture import IMDN\n",
    "from data.custom_dataset import ThermalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a586cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Configuration for 2x upscaling:\n",
      "   ðŸ“ Dataset: /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2\n",
      "   ðŸ‹ï¸ Pretrained: /home/kronbii/repos/thermal-super-resolution/checkpoints/pretrained/IMDN_x2.pth\n",
      "   ðŸ’¾ Checkpoints: checkpoints/_x2\n",
      "   ðŸ§  Memory optimized: Batch=16, Patch=192, Accumulation=3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Configuration - set these parameters directly\n",
    "SCALE = 2\n",
    "DATASET_DIR = f'/home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x{SCALE}'  # Auto-update based on scale\n",
    "PRETRAINED_MODEL_DIR = f'/home/kronbii/repos/thermal-super-resolution/checkpoints/pretrained/IMDN_x{SCALE}.pth'\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 16\n",
    "LR = 2e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Training strategy\n",
    "GRADUAL_UNFREEZE = True\n",
    "FREEZE_EPOCHS = 10\n",
    "\n",
    "# Loss function weights\n",
    "L1_WEIGHT = 1.0\n",
    "GRADIENT_WEIGHT = 0.08\n",
    "THERMAL_WEIGHT = 0.03\n",
    "\n",
    "# System settings\n",
    "NUM_WORKERS = 6\n",
    "DEVICE = 'cuda'\n",
    "MIXED_PRECISION = True\n",
    "\n",
    "# Memory optimization settings\n",
    "PATCH_SIZE = 192\n",
    "GRADIENT_ACCUMULATION_STEPS = 3\n",
    "\n",
    "# Output settings\n",
    "CHECKPOINT_DIR = f'checkpoints/_x{SCALE}'  # Auto-update based on scale\n",
    "LOG_INTERVAL = 50\n",
    "VAL_INTERVAL = 5\n",
    "\n",
    "# Other settings\n",
    "SEED = 42\n",
    "\n",
    "print(f\"ðŸŽ¯ Configuration for {SCALE}x upscaling:\")\n",
    "print(f\"   ðŸ“ Dataset: {DATASET_DIR}\")\n",
    "print(f\"   ðŸ‹ï¸ Pretrained: {PRETRAINED_MODEL_DIR}\")\n",
    "print(f\"   ðŸ’¾ Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"   ðŸ§  Memory optimized: Batch={BATCH_SIZE}, Patch={PATCH_SIZE}, Accumulation={GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3fdd5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace your current set_random_seed function with this optimized version\n",
    "def set_random_seed(seed=42):\n",
    "    \"\"\"Set random seed with performance optimizations\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Speed optimizations\n",
    "    torch.backends.cudnn.deterministic = False  # Much faster\n",
    "    torch.backends.cudnn.benchmark = True       # Auto-optimize kernels\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True    # RTX 3070 acceleration\n",
    "    torch.backends.cudnn.allow_tf32 = True          # RTX 3070 acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3bf9de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize your ThermalLoss class by pre-registering kernels\n",
    "class ThermalLoss(nn.Module):\n",
    "    def __init__(self, l1_weight=1.0, gradient_weight=0.1, thermal_weight=0.05):\n",
    "        super(ThermalLoss, self).__init__()\n",
    "        self.l1_weight = l1_weight\n",
    "        self.gradient_weight = gradient_weight\n",
    "        self.thermal_weight = thermal_weight\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        \n",
    "        # Pre-register Sobel kernels as buffers (major speed boost)\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        self.register_buffer('sobel_x', sobel_x)\n",
    "        self.register_buffer('sobel_y', sobel_y)\n",
    "        \n",
    "        # Pre-register averaging kernel\n",
    "        kernel = torch.ones(1, 1, 3, 3) / 9.0\n",
    "        self.register_buffer('avg_kernel', kernel)\n",
    "    \n",
    "    def gradient_loss(self, pred, target):\n",
    "        \"\"\"Calculate gradient loss using pre-registered kernels\"\"\"\n",
    "        pred_grad_x = F.conv2d(pred, self.sobel_x, padding=1)\n",
    "        pred_grad_y = F.conv2d(pred, self.sobel_y, padding=1)\n",
    "        target_grad_x = F.conv2d(target, self.sobel_x, padding=1)\n",
    "        target_grad_y = F.conv2d(target, self.sobel_y, padding=1)\n",
    "        \n",
    "        grad_loss = self.l1_loss(pred_grad_x, target_grad_x) + self.l1_loss(pred_grad_y, target_grad_y)\n",
    "        return grad_loss\n",
    "    \n",
    "    def thermal_contrast_loss(self, pred, target):\n",
    "        \"\"\"Loss using pre-registered averaging kernel\"\"\"\n",
    "        pred_mean = F.conv2d(pred, self.avg_kernel, padding=1)\n",
    "        target_mean = F.conv2d(target, self.avg_kernel, padding=1)\n",
    "        \n",
    "        pred_var = F.conv2d((pred - pred_mean)**2, self.avg_kernel, padding=1)\n",
    "        target_var = F.conv2d((target - target_mean)**2, self.avg_kernel, padding=1)\n",
    "        \n",
    "        contrast_loss = self.l1_loss(pred_var, target_var)\n",
    "        return contrast_loss\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        grad = self.gradient_loss(pred, target)\n",
    "        thermal = self.thermal_contrast_loss(pred, target)\n",
    "        \n",
    "        total_loss = (self.l1_weight * l1 + \n",
    "                     self.gradient_weight * grad + \n",
    "                     self.thermal_weight * thermal)\n",
    "        \n",
    "        return total_loss, {'l1': l1.item(), 'gradient': grad.item(), 'thermal': thermal.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55d91488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model, freeze_backbone=True):\n",
    "  \"\"\"Freeze/unfreeze model layers for gradual training\"\"\"\n",
    "  for name, param in model.named_parameters():\n",
    "    if freeze_backbone and not any(layer in name.lower() for layer in ['upsampler', 'lr_conv', 'fea_conv']):\n",
    "      param.requires_grad = False\n",
    "    else:\n",
    "      param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "297c2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(img1, img2, max_val=1.0):\n",
    "  \"\"\"Calculate PSNR between two images\"\"\"\n",
    "  mse = torch.mean((img1 - img2)**2)\n",
    "  if mse == 0:\n",
    "    return float('inf')\n",
    "  return 20 * torch.log10(max_val / torch.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e958504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device, max_batches=50):\n",
    "  \"\"\"Validate the model on validation set\"\"\"\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  total_psnr = 0\n",
    "  num_batches = 0\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (lr, hr) in enumerate(val_loader):\n",
    "      if batch_idx >= max_batches:\n",
    "        break\n",
    "        \n",
    "      lr, hr = lr.to(device), hr.to(device)\n",
    "      \n",
    "      # Forward pass\n",
    "      with autocast():\n",
    "        sr = model(lr)\n",
    "        loss, loss_components = criterion(sr, hr)\n",
    "      \n",
    "      # Calculate PSNR\n",
    "      psnr = calculate_psnr(sr, hr)\n",
    "      \n",
    "      total_loss += loss.item()\n",
    "      total_psnr += psnr.item()\n",
    "      num_batches += 1\n",
    "  \n",
    "  avg_loss = total_loss / num_batches\n",
    "  avg_psnr = total_psnr / num_batches\n",
    "  \n",
    "  return avg_loss, avg_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "620b2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, psnr, checkpoint_dir, is_best=False):\n",
    "  \"\"\"Save model checkpoint\"\"\"\n",
    "  checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    'psnr': psnr,\n",
    "  }\n",
    "  \n",
    "  # Save regular checkpoint\n",
    "  checkpoint_path = os.path.join(checkpoint_dir, f'thermal_epoch_{epoch}.pth')\n",
    "  torch.save(checkpoint, checkpoint_path)\n",
    "  \n",
    "  # Save best model\n",
    "  if is_best:\n",
    "    best_path = os.path.join(checkpoint_dir, 'thermal_best.pth')\n",
    "    torch.save(checkpoint, best_path)\n",
    "    print(f\"ðŸ’« New best model saved! PSNR: {psnr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f70239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model, sample_input):\n",
    "  \"\"\"Print model information\"\"\"\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    output = model(sample_input)\n",
    "  \n",
    "  total_params = sum(p.numel() for p in model.parameters())\n",
    "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  \n",
    "  print(f\"ðŸ“Š Model Information:\")\n",
    "  print(f\"   â€¢ Total parameters: {total_params:,}\")\n",
    "  print(f\"   â€¢ Trainable parameters: {trainable_params:,}\")\n",
    "  print(f\"   â€¢ Input shape: {sample_input.shape}\")\n",
    "  print(f\"   â€¢ Output shape: {output.shape}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc24b9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Debugging dataset paths...\n",
      "DATASET_DIR: /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2\n",
      "\n",
      "ðŸ“ Expected directory structure:\n",
      "   âœ… /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/HR - 10697 files\n",
      "   âœ… /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/LR_bicubic/X2 - 10697 files\n",
      "   âœ… /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/val/HR - 1189 files\n",
      "   âœ… /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/val/LR_bicubic/X2 - 1189 files\n",
      "\n",
      "ðŸ“‚ Actual directory structure:\n",
      "flir_thermal_x2/\n",
      "   val/\n",
      "      HR/\n",
      "         (1189 image files)\n",
      "      LR_bicubic/\n",
      "         X2/\n",
      "            (1189 image files)\n"
     ]
    }
   ],
   "source": [
    "# Debug dataset structure before running training\n",
    "print(\"ðŸ” Debugging dataset paths...\")\n",
    "print(f\"DATASET_DIR: {DATASET_DIR}\")\n",
    "print()\n",
    "\n",
    "# Check expected paths\n",
    "expected_paths = [\n",
    "    os.path.join(DATASET_DIR, 'train', 'HR'),\n",
    "    os.path.join(DATASET_DIR, 'train', f'LR_bicubic', f'X{SCALE}'),\n",
    "    os.path.join(DATASET_DIR, 'val', 'HR'),\n",
    "    os.path.join(DATASET_DIR, 'val', f'LR_bicubic', f'X{SCALE}')\n",
    "]\n",
    "\n",
    "print(\"ðŸ“ Expected directory structure:\")\n",
    "for path in expected_paths:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists:\n",
    "        file_count = len([f for f in os.listdir(path) if f.endswith('.png')])\n",
    "        print(f\"   âœ… {path} - {file_count} files\")\n",
    "    else:\n",
    "        print(f\"   âŒ {path} - NOT FOUND\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Show actual directory structure\n",
    "if os.path.exists(DATASET_DIR):\n",
    "    print(\"ðŸ“‚ Actual directory structure:\")\n",
    "    for root, dirs, files in os.walk(DATASET_DIR):\n",
    "        level = root.replace(DATASET_DIR, '').count(os.sep)\n",
    "        indent = '   ' * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        \n",
    "        # Show image files count\n",
    "        image_files = [f for f in files if f.endswith(('.png', '.jpg'))]\n",
    "        if image_files:\n",
    "            subindent = '   ' * (level + 1)\n",
    "            print(f\"{subindent}({len(image_files)} image files)\")\n",
    "        \n",
    "        if level >= 3:  # Don't go too deep\n",
    "            break\n",
    "else:\n",
    "    print(f\"âŒ Dataset directory doesn't exist: {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab4595e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Debugging ThermalDataset loading...\n",
      "Train HR dir: /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/HR\n",
      "Train LR dir: /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/LR_bicubic/X2\n",
      "Directories exist: HR=True, LR=True\n",
      "ðŸ§ª Testing dataset creation...\n",
      "   âœ… TestTrainOpt created successfully\n",
      "   â€¢ hr_dir: /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/HR\n",
      "   â€¢ lr_dir: /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/LR_bicubic/X2\n",
      "   â€¢ scale: 2\n",
      "   â€¢ phase: train\n",
      "Loaded 10697 thermal images for training\n",
      "   âœ… ThermalDataset created successfully\n",
      "   â€¢ Dataset length: 10697\n",
      "   â€¢ images_hr length: 10697\n",
      "   â€¢ Sample HR files: ['/home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/HR/video-24ysbPEGoEKKDvRt6-frame-000000-4C4FHWxwNaMyohLZt.png', '/home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/HR/video-24ysbPEGoEKKDvRt6-frame-000015-ceXK8kdaSPB6ojqyZ.png', '/home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/HR/video-24ysbPEGoEKKDvRt6-frame-000030-JqzDhfF6nR2wLt5dh.png']\n",
      "   â€¢ images_lr length: 10697\n",
      "   â€¢ Sample LR files: ['/home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/LR_bicubic/X2/video-24ysbPEGoEKKDvRt6-frame-000000-4C4FHWxwNaMyohLZt.png', '/home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/LR_bicubic/X2/video-24ysbPEGoEKKDvRt6-frame-000015-ceXK8kdaSPB6ojqyZ.png', '/home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2/train/LR_bicubic/X2/video-24ysbPEGoEKKDvRt6-frame-000030-JqzDhfF6nR2wLt5dh.png']\n"
     ]
    }
   ],
   "source": [
    "# Debug ThermalDataset loading issue\n",
    "print(\"ðŸ”§ Debugging ThermalDataset loading...\")\n",
    "\n",
    "# Test dataset initialization step by step\n",
    "train_hr_dir = os.path.join(DATASET_DIR, 'train', 'HR')\n",
    "train_lr_dir = os.path.join(DATASET_DIR, 'train', f'LR_bicubic', f'X{SCALE}')\n",
    "\n",
    "print(f\"Train HR dir: {train_hr_dir}\")\n",
    "print(f\"Train LR dir: {train_lr_dir}\")\n",
    "print(f\"Directories exist: HR={os.path.exists(train_hr_dir)}, LR={os.path.exists(train_lr_dir)}\")\n",
    "\n",
    "# Test TrainOpt class\n",
    "class TestTrainOpt:\n",
    "    def __init__(self):\n",
    "        self.scale = SCALE\n",
    "        self.phase = 'train'\n",
    "        self.hr_dir = train_hr_dir\n",
    "        self.lr_dir = train_lr_dir\n",
    "        self.ext = '.png'\n",
    "        self.augment = True\n",
    "        self.thermal_augment = True\n",
    "        self.patch_size = PATCH_SIZE\n",
    "        self.n_colors = 1  \n",
    "        self.rgb_range = 1\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.test_every = 1000\n",
    "\n",
    "print(\"ðŸ§ª Testing dataset creation...\")\n",
    "try:\n",
    "    test_opt = TestTrainOpt()\n",
    "    print(f\"   âœ… TestTrainOpt created successfully\")\n",
    "    print(f\"   â€¢ hr_dir: {test_opt.hr_dir}\")\n",
    "    print(f\"   â€¢ lr_dir: {test_opt.lr_dir}\")\n",
    "    print(f\"   â€¢ scale: {test_opt.scale}\")\n",
    "    print(f\"   â€¢ phase: {test_opt.phase}\")\n",
    "    \n",
    "    # Try creating dataset\n",
    "    test_dataset = ThermalDataset(test_opt)\n",
    "    print(f\"   âœ… ThermalDataset created successfully\")\n",
    "    print(f\"   â€¢ Dataset length: {len(test_dataset)}\")\n",
    "    \n",
    "    # Check if dataset has images_hr attribute\n",
    "    if hasattr(test_dataset, 'images_hr'):\n",
    "        print(f\"   â€¢ images_hr length: {len(test_dataset.images_hr)}\")\n",
    "        if test_dataset.images_hr:\n",
    "            print(f\"   â€¢ Sample HR files: {test_dataset.images_hr[:3]}\")\n",
    "    \n",
    "    if hasattr(test_dataset, 'images_lr'):\n",
    "        print(f\"   â€¢ images_lr length: {len(test_dataset.images_lr) if test_dataset.images_lr else 'None'}\")\n",
    "        if test_dataset.images_lr:\n",
    "            print(f\"   â€¢ Sample LR files: {test_dataset.images_lr[:3]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error creating ThermalDataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1127e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "[INFO] CUDA device selected: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "[INFO] CUDA memory available: 8.2 GB\n",
      "[INFO] STARTING OPTIMIZATION PROCESS\n",
      "[INFO] Scale factor: 2x\n",
      "[INFO] Dataset: /home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x2\n",
      "[INFO] Pretrained model: /home/kronbii/repos/thermal-super-resolution/checkpoints/pretrained/IMDN_x2.pth\n",
      "============================================================\n",
      "[INFO] Loading thermal dataset...\n",
      "Loaded 10697 thermal images for training\n",
      "Loaded 1189 thermal images for testing\n",
      "   â€¢ Training samples: 171152\n",
      "   â€¢ Validation samples: 1189\n",
      "============================================================\n",
      "[INFO] Setting up IMDN model...\n",
      "Loaded 10697 thermal images for training\n",
      "Loaded 1189 thermal images for testing\n",
      "   â€¢ Training samples: 171152\n",
      "   â€¢ Validation samples: 1189\n",
      "============================================================\n",
      "[INFO] Setting up IMDN model...\n",
      "[INFO] Loading pretrained weights from /home/kronbii/repos/thermal-super-resolution/checkpoints/pretrained/IMDN_x2.pth\n",
      "   â€¢ Adapted fea_conv.weight: torch.Size([64, 3, 3, 3]) -> torch.Size([64, 1, 3, 3])\n",
      "   â€¢ Adapted upsampler.0.weight: torch.Size([12, 64, 3, 3]) -> torch.Size([4, 64, 3, 3])\n",
      "   â€¢ Adapted upsampler.0.bias: torch.Size([12]) -> torch.Size([4])\n",
      "[INFO] Successfully adapted pretrained model from RGB to thermal with 2x scaling\n",
      "============================================================\n",
      "ðŸ“Š Model Information:\n",
      "   â€¢ Total parameters: 688,636\n",
      "   â€¢ Trainable parameters: 688,636\n",
      "   â€¢ Input shape: torch.Size([1, 1, 64, 64])\n",
      "   â€¢ Output shape: torch.Size([1, 1, 128, 128])\n",
      "\n",
      "[INFO] Starting with frozen backbone (gradual unfreezing enabled)\n",
      "[INFO] Memory optimization enabled:\n",
      "   â€¢ Batch size: 16\n",
      "   â€¢ Patch size: 192\n",
      "   â€¢ Gradient accumulation: 3\n",
      "   â€¢ Mixed precision: True\n",
      "[INFO] Starting training...\n",
      "\n",
      "[INFO] Loading pretrained weights from /home/kronbii/repos/thermal-super-resolution/checkpoints/pretrained/IMDN_x2.pth\n",
      "   â€¢ Adapted fea_conv.weight: torch.Size([64, 3, 3, 3]) -> torch.Size([64, 1, 3, 3])\n",
      "   â€¢ Adapted upsampler.0.weight: torch.Size([12, 64, 3, 3]) -> torch.Size([4, 64, 3, 3])\n",
      "   â€¢ Adapted upsampler.0.bias: torch.Size([12]) -> torch.Size([4])\n",
      "[INFO] Successfully adapted pretrained model from RGB to thermal with 2x scaling\n",
      "============================================================\n",
      "ðŸ“Š Model Information:\n",
      "   â€¢ Total parameters: 688,636\n",
      "   â€¢ Trainable parameters: 688,636\n",
      "   â€¢ Input shape: torch.Size([1, 1, 64, 64])\n",
      "   â€¢ Output shape: torch.Size([1, 1, 128, 128])\n",
      "\n",
      "[INFO] Starting with frozen backbone (gradual unfreezing enabled)\n",
      "[INFO] Memory optimization enabled:\n",
      "   â€¢ Batch size: 16\n",
      "   â€¢ Patch size: 192\n",
      "   â€¢ Gradient accumulation: 3\n",
      "   â€¢ Mixed precision: True\n",
      "[INFO] Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6009/2129058028.py:253: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() if MIXED_PRECISION and device.type == 'cuda' else None\n",
      "/tmp/ipykernel_6009/2129058028.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/tmp/ipykernel_6009/2129058028.py:297: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 [   0/10697 (  0.0%)] Loss: 0.368198 L1: 0.351932 Grad: 0.202938 Thermal: 0.001028 LR: 2.00e-05\n",
      "Epoch   0 [  50/10697 (  0.5%)] Loss: 0.373922 L1: 0.359015 Grad: 0.186001 Thermal: 0.000886 LR: 2.00e-05\n",
      "Epoch   0 [  50/10697 (  0.5%)] Loss: 0.373922 L1: 0.359015 Grad: 0.186001 Thermal: 0.000886 LR: 2.00e-05\n",
      "Epoch   0 [ 100/10697 (  0.9%)] Loss: 0.349823 L1: 0.333594 Grad: 0.202514 Thermal: 0.000914 LR: 2.00e-05\n",
      "Epoch   0 [ 100/10697 (  0.9%)] Loss: 0.349823 L1: 0.333594 Grad: 0.202514 Thermal: 0.000914 LR: 2.00e-05\n",
      "Epoch   0 [ 150/10697 (  1.4%)] Loss: 0.326370 L1: 0.310758 Grad: 0.194801 Thermal: 0.000950 LR: 2.00e-05\n",
      "Epoch   0 [ 150/10697 (  1.4%)] Loss: 0.326370 L1: 0.310758 Grad: 0.194801 Thermal: 0.000950 LR: 2.00e-05\n",
      "Epoch   0 [ 200/10697 (  1.9%)] Loss: 0.332889 L1: 0.317417 Grad: 0.193048 Thermal: 0.000919 LR: 2.00e-05\n",
      "Epoch   0 [ 200/10697 (  1.9%)] Loss: 0.332889 L1: 0.317417 Grad: 0.193048 Thermal: 0.000919 LR: 2.00e-05\n",
      "Epoch   0 [ 250/10697 (  2.3%)] Loss: 0.329125 L1: 0.313333 Grad: 0.197043 Thermal: 0.000939 LR: 2.00e-05\n",
      "Epoch   0 [ 250/10697 (  2.3%)] Loss: 0.329125 L1: 0.313333 Grad: 0.197043 Thermal: 0.000939 LR: 2.00e-05\n",
      "Epoch   0 [ 300/10697 (  2.8%)] Loss: 0.300279 L1: 0.284571 Grad: 0.196019 Thermal: 0.000886 LR: 2.00e-05\n",
      "Epoch   0 [ 300/10697 (  2.8%)] Loss: 0.300279 L1: 0.284571 Grad: 0.196019 Thermal: 0.000886 LR: 2.00e-05\n",
      "Epoch   0 [ 350/10697 (  3.3%)] Loss: 0.271923 L1: 0.257181 Grad: 0.183955 Thermal: 0.000837 LR: 2.00e-05\n",
      "Epoch   0 [ 350/10697 (  3.3%)] Loss: 0.271923 L1: 0.257181 Grad: 0.183955 Thermal: 0.000837 LR: 2.00e-05\n",
      "Epoch   0 [ 400/10697 (  3.7%)] Loss: 0.219513 L1: 0.205140 Grad: 0.179391 Thermal: 0.000728 LR: 2.00e-05\n",
      "Epoch   0 [ 400/10697 (  3.7%)] Loss: 0.219513 L1: 0.205140 Grad: 0.179391 Thermal: 0.000728 LR: 2.00e-05\n",
      "Epoch   0 [ 450/10697 (  4.2%)] Loss: 0.171665 L1: 0.157917 Grad: 0.171583 Thermal: 0.000732 LR: 2.00e-05\n",
      "Epoch   0 [ 450/10697 (  4.2%)] Loss: 0.171665 L1: 0.157917 Grad: 0.171583 Thermal: 0.000732 LR: 2.00e-05\n",
      "Epoch   0 [ 500/10697 (  4.7%)] Loss: 0.102153 L1: 0.087525 Grad: 0.182605 Thermal: 0.000642 LR: 2.00e-05\n",
      "Epoch   0 [ 500/10697 (  4.7%)] Loss: 0.102153 L1: 0.087525 Grad: 0.182605 Thermal: 0.000642 LR: 2.00e-05\n",
      "Epoch   0 [ 550/10697 (  5.1%)] Loss: 0.082686 L1: 0.065181 Grad: 0.218550 Thermal: 0.000691 LR: 2.00e-05\n",
      "Epoch   0 [ 550/10697 (  5.1%)] Loss: 0.082686 L1: 0.065181 Grad: 0.218550 Thermal: 0.000691 LR: 2.00e-05\n",
      "Epoch   0 [ 600/10697 (  5.6%)] Loss: 0.067836 L1: 0.052976 Grad: 0.185494 Thermal: 0.000697 LR: 2.00e-05\n",
      "Epoch   0 [ 600/10697 (  5.6%)] Loss: 0.067836 L1: 0.052976 Grad: 0.185494 Thermal: 0.000697 LR: 2.00e-05\n",
      "Epoch   0 [ 650/10697 (  6.1%)] Loss: 0.072819 L1: 0.057820 Grad: 0.187216 Thermal: 0.000742 LR: 2.00e-05\n",
      "Epoch   0 [ 650/10697 (  6.1%)] Loss: 0.072819 L1: 0.057820 Grad: 0.187216 Thermal: 0.000742 LR: 2.00e-05\n",
      "Epoch   0 [ 700/10697 (  6.5%)] Loss: 0.066752 L1: 0.051974 Grad: 0.184411 Thermal: 0.000826 LR: 2.00e-05\n",
      "Epoch   0 [ 700/10697 (  6.5%)] Loss: 0.066752 L1: 0.051974 Grad: 0.184411 Thermal: 0.000826 LR: 2.00e-05\n",
      "Epoch   0 [ 750/10697 (  7.0%)] Loss: 0.069535 L1: 0.054547 Grad: 0.187092 Thermal: 0.000690 LR: 2.00e-05\n",
      "Epoch   0 [ 750/10697 (  7.0%)] Loss: 0.069535 L1: 0.054547 Grad: 0.187092 Thermal: 0.000690 LR: 2.00e-05\n",
      "Epoch   0 [ 800/10697 (  7.5%)] Loss: 0.070743 L1: 0.054351 Grad: 0.204583 Thermal: 0.000830 LR: 2.00e-05\n",
      "Epoch   0 [ 800/10697 (  7.5%)] Loss: 0.070743 L1: 0.054351 Grad: 0.204583 Thermal: 0.000830 LR: 2.00e-05\n",
      "Epoch   0 [ 850/10697 (  7.9%)] Loss: 0.072824 L1: 0.055541 Grad: 0.215711 Thermal: 0.000865 LR: 2.00e-05\n",
      "Epoch   0 [ 850/10697 (  7.9%)] Loss: 0.072824 L1: 0.055541 Grad: 0.215711 Thermal: 0.000865 LR: 2.00e-05\n",
      "Epoch   0 [ 900/10697 (  8.4%)] Loss: 0.059101 L1: 0.045287 Grad: 0.172392 Thermal: 0.000773 LR: 2.00e-05\n",
      "Epoch   0 [ 900/10697 (  8.4%)] Loss: 0.059101 L1: 0.045287 Grad: 0.172392 Thermal: 0.000773 LR: 2.00e-05\n",
      "Epoch   0 [ 950/10697 (  8.9%)] Loss: 0.064570 L1: 0.049624 Grad: 0.186563 Thermal: 0.000715 LR: 2.00e-05\n",
      "Epoch   0 [ 950/10697 (  8.9%)] Loss: 0.064570 L1: 0.049624 Grad: 0.186563 Thermal: 0.000715 LR: 2.00e-05\n",
      "Epoch   0 [1000/10697 (  9.3%)] Loss: 0.058574 L1: 0.043971 Grad: 0.182246 Thermal: 0.000763 LR: 2.00e-05\n",
      "Epoch   0 [1000/10697 (  9.3%)] Loss: 0.058574 L1: 0.043971 Grad: 0.182246 Thermal: 0.000763 LR: 2.00e-05\n",
      "Epoch   0 [1050/10697 (  9.8%)] Loss: 0.064186 L1: 0.048189 Grad: 0.199673 Thermal: 0.000762 LR: 2.00e-05\n",
      "Epoch   0 [1050/10697 (  9.8%)] Loss: 0.064186 L1: 0.048189 Grad: 0.199673 Thermal: 0.000762 LR: 2.00e-05\n",
      "Epoch   0 [1100/10697 ( 10.3%)] Loss: 0.057341 L1: 0.041294 Grad: 0.200241 Thermal: 0.000908 LR: 2.00e-05\n",
      "Epoch   0 [1100/10697 ( 10.3%)] Loss: 0.057341 L1: 0.041294 Grad: 0.200241 Thermal: 0.000908 LR: 2.00e-05\n",
      "Epoch   0 [1150/10697 ( 10.8%)] Loss: 0.050285 L1: 0.036329 Grad: 0.174208 Thermal: 0.000654 LR: 2.00e-05\n",
      "Epoch   0 [1150/10697 ( 10.8%)] Loss: 0.050285 L1: 0.036329 Grad: 0.174208 Thermal: 0.000654 LR: 2.00e-05\n",
      "Epoch   0 [1200/10697 ( 11.2%)] Loss: 0.051200 L1: 0.037313 Grad: 0.173315 Thermal: 0.000729 LR: 2.00e-05\n",
      "Epoch   0 [1200/10697 ( 11.2%)] Loss: 0.051200 L1: 0.037313 Grad: 0.173315 Thermal: 0.000729 LR: 2.00e-05\n",
      "Epoch   0 [1250/10697 ( 11.7%)] Loss: 0.055965 L1: 0.040539 Grad: 0.192515 Thermal: 0.000831 LR: 2.00e-05\n",
      "Epoch   0 [1250/10697 ( 11.7%)] Loss: 0.055965 L1: 0.040539 Grad: 0.192515 Thermal: 0.000831 LR: 2.00e-05\n",
      "Epoch   0 [1300/10697 ( 12.2%)] Loss: 0.054349 L1: 0.038310 Grad: 0.200171 Thermal: 0.000845 LR: 2.00e-05\n",
      "Epoch   0 [1300/10697 ( 12.2%)] Loss: 0.054349 L1: 0.038310 Grad: 0.200171 Thermal: 0.000845 LR: 2.00e-05\n",
      "Epoch   0 [1350/10697 ( 12.6%)] Loss: 0.051110 L1: 0.035711 Grad: 0.192181 Thermal: 0.000803 LR: 2.00e-05\n",
      "Epoch   0 [1350/10697 ( 12.6%)] Loss: 0.051110 L1: 0.035711 Grad: 0.192181 Thermal: 0.000803 LR: 2.00e-05\n",
      "Epoch   0 [1400/10697 ( 13.1%)] Loss: 0.043013 L1: 0.029826 Grad: 0.164578 Thermal: 0.000699 LR: 2.00e-05\n",
      "Epoch   0 [1400/10697 ( 13.1%)] Loss: 0.043013 L1: 0.029826 Grad: 0.164578 Thermal: 0.000699 LR: 2.00e-05\n",
      "Epoch   0 [1450/10697 ( 13.6%)] Loss: 0.049505 L1: 0.034922 Grad: 0.182019 Thermal: 0.000735 LR: 2.00e-05\n",
      "Epoch   0 [1450/10697 ( 13.6%)] Loss: 0.049505 L1: 0.034922 Grad: 0.182019 Thermal: 0.000735 LR: 2.00e-05\n",
      "Epoch   0 [1500/10697 ( 14.0%)] Loss: 0.047877 L1: 0.033312 Grad: 0.181785 Thermal: 0.000735 LR: 2.00e-05\n",
      "Epoch   0 [1500/10697 ( 14.0%)] Loss: 0.047877 L1: 0.033312 Grad: 0.181785 Thermal: 0.000735 LR: 2.00e-05\n",
      "Epoch   0 [1550/10697 ( 14.5%)] Loss: 0.042084 L1: 0.029092 Grad: 0.162171 Thermal: 0.000622 LR: 2.00e-05\n",
      "Epoch   0 [1550/10697 ( 14.5%)] Loss: 0.042084 L1: 0.029092 Grad: 0.162171 Thermal: 0.000622 LR: 2.00e-05\n",
      "Epoch   0 [1600/10697 ( 15.0%)] Loss: 0.044164 L1: 0.030198 Grad: 0.174313 Thermal: 0.000719 LR: 2.00e-05\n",
      "Epoch   0 [1600/10697 ( 15.0%)] Loss: 0.044164 L1: 0.030198 Grad: 0.174313 Thermal: 0.000719 LR: 2.00e-05\n",
      "Epoch   0 [1650/10697 ( 15.4%)] Loss: 0.045554 L1: 0.031035 Grad: 0.181182 Thermal: 0.000822 LR: 2.00e-05\n",
      "Epoch   0 [1650/10697 ( 15.4%)] Loss: 0.045554 L1: 0.031035 Grad: 0.181182 Thermal: 0.000822 LR: 2.00e-05\n",
      "Epoch   0 [1700/10697 ( 15.9%)] Loss: 0.044033 L1: 0.030263 Grad: 0.171858 Thermal: 0.000733 LR: 2.00e-05\n",
      "Epoch   0 [1700/10697 ( 15.9%)] Loss: 0.044033 L1: 0.030263 Grad: 0.171858 Thermal: 0.000733 LR: 2.00e-05\n",
      "Epoch   0 [1750/10697 ( 16.4%)] Loss: 0.044068 L1: 0.030227 Grad: 0.172746 Thermal: 0.000703 LR: 2.00e-05\n",
      "Epoch   0 [1750/10697 ( 16.4%)] Loss: 0.044068 L1: 0.030227 Grad: 0.172746 Thermal: 0.000703 LR: 2.00e-05\n",
      "Epoch   0 [1800/10697 ( 16.8%)] Loss: 0.040926 L1: 0.027849 Grad: 0.163198 Thermal: 0.000703 LR: 2.00e-05\n",
      "Epoch   0 [1800/10697 ( 16.8%)] Loss: 0.040926 L1: 0.027849 Grad: 0.163198 Thermal: 0.000703 LR: 2.00e-05\n",
      "Epoch   0 [1850/10697 ( 17.3%)] Loss: 0.042393 L1: 0.029104 Grad: 0.165862 Thermal: 0.000656 LR: 2.00e-05\n",
      "Epoch   0 [1850/10697 ( 17.3%)] Loss: 0.042393 L1: 0.029104 Grad: 0.165862 Thermal: 0.000656 LR: 2.00e-05\n",
      "Epoch   0 [1900/10697 ( 17.8%)] Loss: 0.041040 L1: 0.028029 Grad: 0.162384 Thermal: 0.000668 LR: 2.00e-05\n",
      "Epoch   0 [1900/10697 ( 17.8%)] Loss: 0.041040 L1: 0.028029 Grad: 0.162384 Thermal: 0.000668 LR: 2.00e-05\n",
      "Epoch   0 [1950/10697 ( 18.2%)] Loss: 0.037950 L1: 0.025887 Grad: 0.150591 Thermal: 0.000544 LR: 2.00e-05\n",
      "Epoch   0 [1950/10697 ( 18.2%)] Loss: 0.037950 L1: 0.025887 Grad: 0.150591 Thermal: 0.000544 LR: 2.00e-05\n",
      "Epoch   0 [2000/10697 ( 18.7%)] Loss: 0.035995 L1: 0.024274 Grad: 0.146299 Thermal: 0.000592 LR: 2.00e-05\n",
      "Epoch   0 [2000/10697 ( 18.7%)] Loss: 0.035995 L1: 0.024274 Grad: 0.146299 Thermal: 0.000592 LR: 2.00e-05\n",
      "Epoch   0 [2050/10697 ( 19.2%)] Loss: 0.037381 L1: 0.025228 Grad: 0.151694 Thermal: 0.000592 LR: 2.00e-05\n",
      "Epoch   0 [2050/10697 ( 19.2%)] Loss: 0.037381 L1: 0.025228 Grad: 0.151694 Thermal: 0.000592 LR: 2.00e-05\n",
      "Epoch   0 [2100/10697 ( 19.6%)] Loss: 0.042564 L1: 0.028714 Grad: 0.172875 Thermal: 0.000690 LR: 2.00e-05\n",
      "Epoch   0 [2100/10697 ( 19.6%)] Loss: 0.042564 L1: 0.028714 Grad: 0.172875 Thermal: 0.000690 LR: 2.00e-05\n",
      "Epoch   0 [2150/10697 ( 20.1%)] Loss: 0.040836 L1: 0.027407 Grad: 0.167604 Thermal: 0.000683 LR: 2.00e-05\n",
      "Epoch   0 [2150/10697 ( 20.1%)] Loss: 0.040836 L1: 0.027407 Grad: 0.167604 Thermal: 0.000683 LR: 2.00e-05\n",
      "Epoch   0 [2200/10697 ( 20.6%)] Loss: 0.040216 L1: 0.027168 Grad: 0.162854 Thermal: 0.000686 LR: 2.00e-05\n",
      "Epoch   0 [2200/10697 ( 20.6%)] Loss: 0.040216 L1: 0.027168 Grad: 0.162854 Thermal: 0.000686 LR: 2.00e-05\n",
      "Epoch   0 [2250/10697 ( 21.0%)] Loss: 0.038855 L1: 0.026172 Grad: 0.158311 Thermal: 0.000603 LR: 2.00e-05\n",
      "Epoch   0 [2250/10697 ( 21.0%)] Loss: 0.038855 L1: 0.026172 Grad: 0.158311 Thermal: 0.000603 LR: 2.00e-05\n",
      "Epoch   0 [2300/10697 ( 21.5%)] Loss: 0.039368 L1: 0.026230 Grad: 0.163955 Thermal: 0.000695 LR: 2.00e-05\n",
      "Epoch   0 [2300/10697 ( 21.5%)] Loss: 0.039368 L1: 0.026230 Grad: 0.163955 Thermal: 0.000695 LR: 2.00e-05\n",
      "Epoch   0 [2350/10697 ( 22.0%)] Loss: 0.038674 L1: 0.026004 Grad: 0.158132 Thermal: 0.000632 LR: 2.00e-05\n",
      "Epoch   0 [2350/10697 ( 22.0%)] Loss: 0.038674 L1: 0.026004 Grad: 0.158132 Thermal: 0.000632 LR: 2.00e-05\n",
      "Epoch   0 [2400/10697 ( 22.4%)] Loss: 0.040520 L1: 0.026918 Grad: 0.169749 Thermal: 0.000739 LR: 2.00e-05\n",
      "Epoch   0 [2400/10697 ( 22.4%)] Loss: 0.040520 L1: 0.026918 Grad: 0.169749 Thermal: 0.000739 LR: 2.00e-05\n",
      "Epoch   0 [2450/10697 ( 22.9%)] Loss: 0.041710 L1: 0.027971 Grad: 0.171471 Thermal: 0.000690 LR: 2.00e-05\n",
      "Epoch   0 [2450/10697 ( 22.9%)] Loss: 0.041710 L1: 0.027971 Grad: 0.171471 Thermal: 0.000690 LR: 2.00e-05\n",
      "Epoch   0 [2500/10697 ( 23.4%)] Loss: 0.038507 L1: 0.025998 Grad: 0.156127 Thermal: 0.000628 LR: 2.00e-05\n",
      "Epoch   0 [2500/10697 ( 23.4%)] Loss: 0.038507 L1: 0.025998 Grad: 0.156127 Thermal: 0.000628 LR: 2.00e-05\n",
      "Epoch   0 [2550/10697 ( 23.8%)] Loss: 0.039882 L1: 0.026614 Grad: 0.165624 Thermal: 0.000608 LR: 2.00e-05\n",
      "Epoch   0 [2550/10697 ( 23.8%)] Loss: 0.039882 L1: 0.026614 Grad: 0.165624 Thermal: 0.000608 LR: 2.00e-05\n",
      "Epoch   0 [2600/10697 ( 24.3%)] Loss: 0.037060 L1: 0.024433 Grad: 0.157595 Thermal: 0.000630 LR: 2.00e-05\n",
      "Epoch   0 [2600/10697 ( 24.3%)] Loss: 0.037060 L1: 0.024433 Grad: 0.157595 Thermal: 0.000630 LR: 2.00e-05\n",
      "Epoch   0 [2650/10697 ( 24.8%)] Loss: 0.033587 L1: 0.022344 Grad: 0.140309 Thermal: 0.000587 LR: 2.00e-05\n",
      "Epoch   0 [2650/10697 ( 24.8%)] Loss: 0.033587 L1: 0.022344 Grad: 0.140309 Thermal: 0.000587 LR: 2.00e-05\n",
      "Epoch   0 [2700/10697 ( 25.2%)] Loss: 0.035196 L1: 0.023167 Grad: 0.150127 Thermal: 0.000606 LR: 2.00e-05\n",
      "Epoch   0 [2700/10697 ( 25.2%)] Loss: 0.035196 L1: 0.023167 Grad: 0.150127 Thermal: 0.000606 LR: 2.00e-05\n",
      "Epoch   0 [2750/10697 ( 25.7%)] Loss: 0.038388 L1: 0.025564 Grad: 0.160055 Thermal: 0.000632 LR: 2.00e-05\n",
      "Epoch   0 [2750/10697 ( 25.7%)] Loss: 0.038388 L1: 0.025564 Grad: 0.160055 Thermal: 0.000632 LR: 2.00e-05\n",
      "Epoch   0 [2800/10697 ( 26.2%)] Loss: 0.033131 L1: 0.021805 Grad: 0.141391 Thermal: 0.000493 LR: 2.00e-05\n",
      "Epoch   0 [2800/10697 ( 26.2%)] Loss: 0.033131 L1: 0.021805 Grad: 0.141391 Thermal: 0.000493 LR: 2.00e-05\n",
      "Epoch   0 [2850/10697 ( 26.6%)] Loss: 0.031833 L1: 0.020976 Grad: 0.135512 Thermal: 0.000522 LR: 2.00e-05\n",
      "Epoch   0 [2850/10697 ( 26.6%)] Loss: 0.031833 L1: 0.020976 Grad: 0.135512 Thermal: 0.000522 LR: 2.00e-05\n",
      "Epoch   0 [2900/10697 ( 27.1%)] Loss: 0.033500 L1: 0.022156 Grad: 0.141607 Thermal: 0.000532 LR: 2.00e-05\n",
      "Epoch   0 [2900/10697 ( 27.1%)] Loss: 0.033500 L1: 0.022156 Grad: 0.141607 Thermal: 0.000532 LR: 2.00e-05\n",
      "Epoch   0 [2950/10697 ( 27.6%)] Loss: 0.032960 L1: 0.021874 Grad: 0.138381 Thermal: 0.000495 LR: 2.00e-05\n",
      "Epoch   0 [2950/10697 ( 27.6%)] Loss: 0.032960 L1: 0.021874 Grad: 0.138381 Thermal: 0.000495 LR: 2.00e-05\n",
      "Epoch   0 [3000/10697 ( 28.0%)] Loss: 0.033322 L1: 0.021891 Grad: 0.142681 Thermal: 0.000573 LR: 2.00e-05\n",
      "Epoch   0 [3000/10697 ( 28.0%)] Loss: 0.033322 L1: 0.021891 Grad: 0.142681 Thermal: 0.000573 LR: 2.00e-05\n",
      "Epoch   0 [3050/10697 ( 28.5%)] Loss: 0.038614 L1: 0.025671 Grad: 0.161551 Thermal: 0.000631 LR: 2.00e-05\n",
      "Epoch   0 [3050/10697 ( 28.5%)] Loss: 0.038614 L1: 0.025671 Grad: 0.161551 Thermal: 0.000631 LR: 2.00e-05\n",
      "Epoch   0 [3100/10697 ( 29.0%)] Loss: 0.034937 L1: 0.023195 Grad: 0.146543 Thermal: 0.000604 LR: 2.00e-05\n",
      "Epoch   0 [3100/10697 ( 29.0%)] Loss: 0.034937 L1: 0.023195 Grad: 0.146543 Thermal: 0.000604 LR: 2.00e-05\n",
      "Epoch   0 [3150/10697 ( 29.4%)] Loss: 0.032791 L1: 0.021429 Grad: 0.141809 Thermal: 0.000577 LR: 2.00e-05\n",
      "Epoch   0 [3150/10697 ( 29.4%)] Loss: 0.032791 L1: 0.021429 Grad: 0.141809 Thermal: 0.000577 LR: 2.00e-05\n",
      "Epoch   0 [3200/10697 ( 29.9%)] Loss: 0.031754 L1: 0.020989 Grad: 0.134384 Thermal: 0.000490 LR: 2.00e-05\n",
      "Epoch   0 [3200/10697 ( 29.9%)] Loss: 0.031754 L1: 0.020989 Grad: 0.134384 Thermal: 0.000490 LR: 2.00e-05\n",
      "Epoch   0 [3250/10697 ( 30.4%)] Loss: 0.036651 L1: 0.024017 Grad: 0.157682 Thermal: 0.000644 LR: 2.00e-05\n",
      "Epoch   0 [3250/10697 ( 30.4%)] Loss: 0.036651 L1: 0.024017 Grad: 0.157682 Thermal: 0.000644 LR: 2.00e-05\n",
      "Epoch   0 [3300/10697 ( 30.8%)] Loss: 0.030137 L1: 0.019814 Grad: 0.128849 Thermal: 0.000482 LR: 2.00e-05\n",
      "Epoch   0 [3300/10697 ( 30.8%)] Loss: 0.030137 L1: 0.019814 Grad: 0.128849 Thermal: 0.000482 LR: 2.00e-05\n",
      "Epoch   0 [3350/10697 ( 31.3%)] Loss: 0.035601 L1: 0.023283 Grad: 0.153756 Thermal: 0.000588 LR: 2.00e-05\n",
      "Epoch   0 [3350/10697 ( 31.3%)] Loss: 0.035601 L1: 0.023283 Grad: 0.153756 Thermal: 0.000588 LR: 2.00e-05\n",
      "Epoch   0 [3400/10697 ( 31.8%)] Loss: 0.038102 L1: 0.024984 Grad: 0.163721 Thermal: 0.000687 LR: 2.00e-05\n",
      "Epoch   0 [3400/10697 ( 31.8%)] Loss: 0.038102 L1: 0.024984 Grad: 0.163721 Thermal: 0.000687 LR: 2.00e-05\n",
      "Epoch   0 [3450/10697 ( 32.3%)] Loss: 0.036740 L1: 0.024295 Grad: 0.155323 Thermal: 0.000633 LR: 2.00e-05\n",
      "Epoch   0 [3450/10697 ( 32.3%)] Loss: 0.036740 L1: 0.024295 Grad: 0.155323 Thermal: 0.000633 LR: 2.00e-05\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "set_random_seed(SEED)\n",
    "\n",
    "# Setup device\n",
    "if DEVICE == 'auto':\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "\tdevice = torch.device(DEVICE)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "# Check if CUDA setup was successful\n",
    "if device.type == 'cuda':\n",
    "\tprint(f\"[INFO] CUDA device selected: {torch.cuda.get_device_name(device)}\")\n",
    "\tprint(f\"[INFO] CUDA memory available: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "\tprint(\"[WARNING] Using CPU - training will be significantly slower\")\n",
    "\n",
    "print(\"[INFO] STARTING OPTIMIZATION PROCESS\")\n",
    "print(f\"[INFO] Scale factor: {SCALE}x\")\n",
    "print(f\"[INFO] Dataset: {DATASET_DIR}\")\n",
    "print(f\"[INFO] Pretrained model: {PRETRAINED_MODEL_DIR}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Setup datasets\n",
    "print(\"[INFO] Loading thermal dataset...\")\n",
    "\n",
    "# Training dataset\n",
    "train_hr_dir = os.path.join(DATASET_DIR, 'train', 'HR')\n",
    "train_lr_dir = os.path.join(DATASET_DIR, 'train', f'LR_bicubic', f'X{SCALE}')\n",
    "\n",
    "# Create options object for ThermalDataset with FIXED repeat calculation\n",
    "class TrainOpt:\n",
    "\tdef __init__(self):\n",
    "\t\tself.scale = SCALE\n",
    "\t\tself.phase = 'train'\n",
    "\t\tself.hr_dir = train_hr_dir\n",
    "\t\tself.lr_dir = train_lr_dir\n",
    "\t\tself.ext = '.png'\n",
    "\t\tself.augment = True\n",
    "\t\tself.thermal_augment = True\n",
    "\t\tself.patch_size = PATCH_SIZE\n",
    "\t\tself.n_colors = 1\n",
    "\t\tself.rgb_range = 1\n",
    "\t\tself.batch_size = BATCH_SIZE\n",
    "\t\tself.test_every = max(1000, len(os.listdir(train_hr_dir)))  # Fix repeat calculation\n",
    "\n",
    "train_dataset = ThermalDataset(TrainOpt())\n",
    "\n",
    "# Validation dataset  \n",
    "val_hr_dir = os.path.join(DATASET_DIR, 'val', 'HR')\n",
    "val_lr_dir = os.path.join(DATASET_DIR, 'val', f'LR_bicubic', f'X{SCALE}')\n",
    "\n",
    "# Create options object for validation dataset\n",
    "class ValOpt:\n",
    "\tdef __init__(self):\n",
    "\t\tself.scale = SCALE\n",
    "\t\tself.phase = 'val'\n",
    "\t\tself.hr_dir = val_hr_dir\n",
    "\t\tself.lr_dir = val_lr_dir\n",
    "\t\tself.ext = '.png'\n",
    "\t\tself.augment = False\n",
    "\t\tself.thermal_augment = False\n",
    "\t\tself.patch_size = PATCH_SIZE\n",
    "\t\tself.n_colors = 1\n",
    "\t\tself.rgb_range = 1\n",
    "\t\tself.batch_size = BATCH_SIZE\n",
    "\t\tself.test_every = 1000\n",
    "\n",
    "val_dataset = ThermalDataset(ValOpt())\n",
    "\n",
    "print(f\"   â€¢ Training samples: {len(train_dataset)}\")\n",
    "print(f\"   â€¢ Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Quick fix if training samples still 0\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"[ERROR] Training dataset length is 0, fixing repeat calculation...\")\n",
    "    # Manually set repeat to 1 for training dataset\n",
    "    train_dataset.repeat = 1\n",
    "    print(f\"   â€¢ Fixed training samples: {len(train_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,    # Keeps workers alive between epochs\n",
    "    prefetch_factor=3,          # More aggressive prefetching\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Setup model\n",
    "print(\"[INFO] Setting up IMDN model...\")\n",
    "model = IMDN(upscale=SCALE, in_nc=1, out_nc=1)  # Single channel for thermal\n",
    "\n",
    "# Load pretrained weights\n",
    "if os.path.exists(PRETRAINED_MODEL_DIR):\n",
    "\tprint(f\"[INFO] Loading pretrained weights from {PRETRAINED_MODEL_DIR}\")\n",
    "\ttry:\n",
    "\t\tcheckpoint = torch.load(PRETRAINED_MODEL_DIR, map_location='cpu', weights_only=True)\n",
    "\texcept:\n",
    "\t\t# Fallback for older PyTorch versions\n",
    "\t\tcheckpoint = torch.load(PRETRAINED_MODEL_DIR, map_location='cpu')\n",
    "\n",
    "\t# Extract state dict if it's wrapped in a checkpoint\n",
    "\tif isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "\t\tstate_dict = checkpoint['model_state_dict']\n",
    "\telse:\n",
    "\t\tstate_dict = checkpoint\n",
    "\n",
    "\t# Remove 'module.' prefix if present (from DataParallel training)\n",
    "\tif any(key.startswith('module.') for key in state_dict.keys()):\n",
    "\t\tstate_dict = {key.replace('module.', ''): value for key, value in state_dict.items()}\n",
    "\n",
    "\t# Universal weight adaptation for any scale factor\n",
    "\tadapted_state_dict = {}\n",
    "\ttarget_upsampler_channels = SCALE * SCALE  # For thermal: 1 channel * scale^2\n",
    "\t\n",
    "\tfor name, param in state_dict.items():\n",
    "\t\tif name == 'fea_conv.weight' and param.shape[1] == 3:  \n",
    "\t\t\t# RGB input layer -> thermal input layer (3->1 channel)\n",
    "\t\t\tadapted_param = param.mean(dim=1, keepdim=True)\n",
    "\t\t\tadapted_state_dict[name] = adapted_param\n",
    "\t\t\tprint(f\"   â€¢ Adapted {name}: {param.shape} -> {adapted_param.shape}\")\n",
    "\t\t\n",
    "\t\telif name.startswith('upsampler.') and 'weight' in name:\n",
    "\t\t\t# Handle upsampler weight - adapt from RGB to thermal\n",
    "\t\t\tsource_channels = param.shape[0]  # RGB: 3 * source_scale^2\n",
    "\t\t\t\n",
    "\t\t\tif source_channels % 3 == 0:  # Confirm it's RGB (divisible by 3)\n",
    "\t\t\t\tsource_scale_sq = source_channels // 3  # Get source scale^2\n",
    "\t\t\t\t\n",
    "\t\t\t\tif source_scale_sq == target_upsampler_channels:\n",
    "\t\t\t\t\t# Same scale: just average RGB channels to get thermal\n",
    "\t\t\t\t\tparam_reshaped = param.view(3, target_upsampler_channels, param.shape[1], param.shape[2], param.shape[3])\n",
    "\t\t\t\t\tadapted_param = param_reshaped.mean(dim=0)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Different scales: need to adapt the scale\n",
    "\t\t\t\t\tif target_upsampler_channels <= source_scale_sq:\n",
    "\t\t\t\t\t\t# Target scale is smaller: downsample\n",
    "\t\t\t\t\t\tparam_reshaped = param.view(3, source_scale_sq, param.shape[1], param.shape[2], param.shape[3])\n",
    "\t\t\t\t\t\trgb_avg = param_reshaped.mean(dim=0)  # Average RGB channels first\n",
    "\t\t\t\t\t\tadapted_param = rgb_avg[:target_upsampler_channels]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Target scale is larger: upsample by repeating\n",
    "\t\t\t\t\t\tparam_reshaped = param.view(3, source_scale_sq, param.shape[1], param.shape[2], param.shape[3])\n",
    "\t\t\t\t\t\trgb_avg = param_reshaped.mean(dim=0)  # Average RGB channels first\n",
    "\t\t\t\t\t\trepeat_factor = target_upsampler_channels // source_scale_sq\n",
    "\t\t\t\t\t\tremainder = target_upsampler_channels % source_scale_sq\n",
    "\t\t\t\t\t\tadapted_param = rgb_avg.repeat(repeat_factor, 1, 1, 1)\n",
    "\t\t\t\t\t\tif remainder > 0:\n",
    "\t\t\t\t\t\t\tadapted_param = torch.cat([adapted_param, rgb_avg[:remainder]], dim=0)\n",
    "\t\t\t\t\n",
    "\t\t\t\tadapted_state_dict[name] = adapted_param\n",
    "\t\t\t\tprint(f\"   â€¢ Adapted {name}: {param.shape} -> {adapted_param.shape}\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tadapted_state_dict[name] = param\n",
    "\t\t\n",
    "\t\telif name.startswith('upsampler.') and 'bias' in name:\n",
    "\t\t\t# Handle upsampler bias - adapt from RGB to thermal\n",
    "\t\t\tsource_channels = param.shape[0]  # RGB: 3 * source_scale^2\n",
    "\t\t\t\n",
    "\t\t\tif source_channels % 3 == 0:  # Confirm it's RGB (divisible by 3)\n",
    "\t\t\t\tsource_scale_sq = source_channels // 3  # Get source scale^2\n",
    "\t\t\t\t\n",
    "\t\t\t\tif source_scale_sq == target_upsampler_channels:\n",
    "\t\t\t\t\t# Same scale: just average RGB channels to get thermal\n",
    "\t\t\t\t\tparam_reshaped = param.view(3, target_upsampler_channels)\n",
    "\t\t\t\t\tadapted_param = param_reshaped.mean(dim=0)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Different scales: need to adapt the scale\n",
    "\t\t\t\t\tif target_upsampler_channels <= source_scale_sq:\n",
    "\t\t\t\t\t\t# Target scale is smaller: downsample\n",
    "\t\t\t\t\t\tparam_reshaped = param.view(3, source_scale_sq)\n",
    "\t\t\t\t\t\trgb_avg = param_reshaped.mean(dim=0)  # Average RGB channels first\n",
    "\t\t\t\t\t\tadapted_param = rgb_avg[:target_upsampler_channels]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Target scale is larger: upsample by repeating\n",
    "\t\t\t\t\t\tparam_reshaped = param.view(3, source_scale_sq)\n",
    "\t\t\t\t\t\trgb_avg = param_reshaped.mean(dim=0)  # Average RGB channels first\n",
    "\t\t\t\t\t\trepeat_factor = target_upsampler_channels // source_scale_sq\n",
    "\t\t\t\t\t\tremainder = target_upsampler_channels % source_scale_sq\n",
    "\t\t\t\t\t\tadapted_param = rgb_avg.repeat(repeat_factor)\n",
    "\t\t\t\t\t\tif remainder > 0:\n",
    "\t\t\t\t\t\t\tadapted_param = torch.cat([adapted_param, rgb_avg[:remainder]], dim=0)\n",
    "\t\t\t\t\n",
    "\t\t\t\tadapted_state_dict[name] = adapted_param\n",
    "\t\t\t\tprint(f\"   â€¢ Adapted {name}: {param.shape} -> {adapted_param.shape}\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tadapted_state_dict[name] = param\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\t# All other layers: keep as is\n",
    "\t\t\tadapted_state_dict[name] = param\n",
    "\n",
    "\t# Load adapted weights\n",
    "\tmissing_keys, unexpected_keys = model.load_state_dict(adapted_state_dict, strict=False)\n",
    "\tif missing_keys:\n",
    "\t\tprint(f\"[ERROR] Missing keys: {missing_keys}\")\n",
    "\tif unexpected_keys:\n",
    "\t\tprint(f\"[ERROR] Unexpected keys: {unexpected_keys}\")\n",
    "\t\n",
    "\tprint(f\"[INFO] Successfully adapted pretrained model from RGB to thermal with {SCALE}x scaling\")\n",
    "else:\n",
    "\tprint(f\"[ERROR] Pretrained model not found at {PRETRAINED_MODEL_DIR}\")\n",
    "\tprint(\"[ERROR] Training from scratch (this will take much longer)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = model.to(device)\n",
    "model = model.to(memory_format=torch.channels_last)  # Memory layout optimization\n",
    "\n",
    "# Print model info\n",
    "sample_input = torch.randn(1, 1, 64, 64).to(device)\n",
    "print_model_info(model, sample_input)\n",
    "\n",
    "# Setup loss function\n",
    "criterion = ThermalLoss(\n",
    "\tl1_weight=L1_WEIGHT,\n",
    "\tgradient_weight=GRADIENT_WEIGHT,\n",
    "\tthermal_weight=THERMAL_WEIGHT\n",
    ").to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.AdamW(\n",
    "\tmodel.parameters(),\n",
    "\tlr=LR,\n",
    "\tweight_decay=WEIGHT_DECAY,\n",
    "\tbetas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "\toptimizer,\n",
    "\tT_max=EPOCHS,\n",
    "\teta_min=LR * 0.01\n",
    ")\n",
    "\n",
    "# Setup mixed precision training\n",
    "scaler = GradScaler() if MIXED_PRECISION and device.type == 'cuda' else None\n",
    "\n",
    "# Gradual unfreezing setup\n",
    "if GRADUAL_UNFREEZE:\n",
    "\tprint(\"[INFO] Starting with frozen backbone (gradual unfreezing enabled)\")\n",
    "\tfreeze_layers(model, freeze_backbone=True)\n",
    "\n",
    "# Memory optimization\n",
    "print(\"[INFO] Memory optimization enabled:\")\n",
    "print(f\"   â€¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   â€¢ Patch size: {PATCH_SIZE}\")\n",
    "print(f\"   â€¢ Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"   â€¢ Mixed precision: {MIXED_PRECISION}\")\n",
    "\n",
    "# Training loop with gradient accumulation\n",
    "print(\"[INFO] Starting training...\")\n",
    "print()\n",
    "\n",
    "best_psnr = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\t# Gradual unfreezing\n",
    "\tif GRADUAL_UNFREEZE and epoch == FREEZE_EPOCHS:\n",
    "\t\tprint(\"[INFO] Unfreezing backbone layers\")\n",
    "\t\tfreeze_layers(model, freeze_backbone=False)\n",
    "\t\t# Reduce learning rate when unfreezing\n",
    "\t\tfor param_group in optimizer.param_groups:\n",
    "\t\t\tparam_group['lr'] *= 0.5\n",
    "\n",
    "\tmodel.train()\n",
    "\tepoch_loss = 0\n",
    "\tepoch_l1 = 0\n",
    "\tepoch_gradient = 0\n",
    "\tepoch_thermal = 0\n",
    "\t\n",
    "\t# Gradient accumulation setup\n",
    "\toptimizer.zero_grad()\n",
    "\n",
    "\tfor batch_idx, (lr, hr) in enumerate(train_loader):\n",
    "\t\tlr, hr = lr.to(device), hr.to(device)\n",
    "\t\t\n",
    "\t\t# Forward pass with mixed precision\n",
    "\t\tif scaler is not None:\n",
    "\t\t\twith autocast():\n",
    "\t\t\t\tsr = model(lr)\n",
    "\t\t\t\tloss, loss_components = criterion(sr, hr)\n",
    "\t\t\t\tloss = loss / GRADIENT_ACCUMULATION_STEPS  # Scale loss for accumulation\n",
    "\t\t\n",
    "\t\t\t# Backward pass\n",
    "\t\t\tscaler.scale(loss).backward()\n",
    "\t\telse:\n",
    "\t\t\tsr = model(lr)\n",
    "\t\t\tloss, loss_components = criterion(sr, hr)\n",
    "\t\t\tloss = loss / GRADIENT_ACCUMULATION_STEPS  # Scale loss for accumulation\n",
    "\t\t\tloss.backward()\n",
    "\t\t\n",
    "\t\t# Accumulate losses (scale back for logging)\n",
    "\t\tepoch_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "\t\tepoch_l1 += loss_components['l1']\n",
    "\t\tepoch_gradient += loss_components['gradient']\n",
    "\t\tepoch_thermal += loss_components['thermal']\n",
    "\t\t\n",
    "\t\t# Update weights every GRADIENT_ACCUMULATION_STEPS\n",
    "\t\tif (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "\t\t\tif scaler is not None:\n",
    "\t\t\t\tscaler.step(optimizer)\n",
    "\t\t\t\tscaler.update()\n",
    "\t\t\telse:\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\n",
    "\t\t\t# Clear cache periodically\n",
    "\t\t\tif batch_idx % 20 == 0:\n",
    "\t\t\t\ttorch.cuda.empty_cache()\n",
    "\t\t\n",
    "\t\t# Logging\n",
    "\t\tif batch_idx % LOG_INTERVAL == 0:\n",
    "\t\t\tprogress = 100.0 * batch_idx / len(train_loader)\n",
    "\t\t\tcurrent_lr = optimizer.param_groups[0]['lr']\n",
    "\t\t\tprint(f\"Epoch {epoch:3d} [{batch_idx:4d}/{len(train_loader)} ({progress:5.1f}%)] \"\n",
    "\t\t\t\t  f\"Loss: {loss.item() * GRADIENT_ACCUMULATION_STEPS:.6f} L1: {loss_components['l1']:.6f} \"\n",
    "\t\t\t\t  f\"Grad: {loss_components['gradient']:.6f} Thermal: {loss_components['thermal']:.6f} \"\n",
    "\t\t\t\t  f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "\t# Update learning rate\n",
    "\tscheduler.step()\n",
    "\n",
    "\t# Calculate epoch averages\n",
    "\tavg_loss = epoch_loss / len(train_loader)\n",
    "\tavg_l1 = epoch_l1 / len(train_loader)\n",
    "\tavg_gradient = epoch_gradient / len(train_loader)\n",
    "\tavg_thermal = epoch_thermal / len(train_loader)\n",
    "\n",
    "\t# Validation\n",
    "\tval_loss, val_psnr = 0, 0\n",
    "\tif epoch % VAL_INTERVAL == 0:\n",
    "\t\tval_loss, val_psnr = validate_model(model, val_loader, criterion, device)\n",
    "\t\t\n",
    "\t\t# Save checkpoint if best\n",
    "\t\tis_best = val_psnr > best_psnr\n",
    "\t\tif is_best:\n",
    "\t\t\tbest_psnr = val_psnr\n",
    "\t\t\n",
    "\t\tsave_checkpoint(model, optimizer, epoch, val_loss, val_psnr, CHECKPOINT_DIR, is_best)\n",
    "\n",
    "\t# Epoch summary\n",
    "\telapsed = time.time() - start_time\n",
    "\tprint(f\"Epoch {epoch:3d} Summary: Loss={avg_loss:.6f} (L1:{avg_l1:.4f}, Grad:{avg_gradient:.4f}, Thermal:{avg_thermal:.4f}) \"\n",
    "\t\t  f\"Val_PSNR={val_psnr:.2f}dB Best={best_psnr:.2f}dB Time={elapsed/60:.1f}min\")\n",
    "\tprint(\"-\" * 100)\n",
    "\n",
    "# Final summary\n",
    "total_time = time.time() - start_time\n",
    "print()\n",
    "print(\"[INFO] Training completed!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"[INFO] Best validation PSNR: {best_psnr:.2f} dB\")\n",
    "print(f\"[INFO] Total training time: {total_time/3600:.2f} hours\")\n",
    "print(f\"[INFO] Best model saved at: {os.path.join(CHECKPOINT_DIR, 'thermal_best.pth')}\")\n",
    "print()\n",
    "print(\"[INFO] Your thermal super-resolution model is ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
