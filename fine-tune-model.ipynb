{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586134b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Add the current directory to path to import local modules\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from model.architecture import IMDN\n",
    "from data.custom_dataset import ThermalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - set these parameters directly\n",
    "SCALE = 2\n",
    "DATASET_DIR = f'/home/kronbii/repos/thermal-super-resolution/datasets/flir_thermal_x{SCALE}'  # Auto-update based on scale\n",
    "PRETRAINED_MODEL_DIR = f'/home/kronbii/repos/thermal-super-resolution/checkpoints/pretrained/IMDN_x{SCALE}.pth'\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 60\n",
    "BATCH_SIZE = 12\n",
    "LR = 2.5e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Training strategy\n",
    "GRADUAL_UNFREEZE = True\n",
    "FREEZE_EPOCHS = 8\n",
    "\n",
    "# Loss function weights\n",
    "L1_WEIGHT = 1.0\n",
    "GRADIENT_WEIGHT = 0.06\n",
    "THERMAL_WEIGHT = 0.02\n",
    "\n",
    "# System settings\n",
    "NUM_WORKERS = 6\n",
    "DEVICE = 'cuda'\n",
    "MIXED_PRECISION = True\n",
    "\n",
    "# Memory optimization settings\n",
    "PATCH_SIZE = 160\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "\n",
    "# Output settings\n",
    "CHECKPOINT_DIR = f'checkpoints/_x{SCALE}'  # Auto-update based on scale\n",
    "LOG_INTERVAL = 50\n",
    "VAL_INTERVAL = 3\n",
    "\n",
    "# Other settings\n",
    "SEED = 42\n",
    "\n",
    "print(f\"ðŸŽ¯ Configuration for {SCALE}x upscaling:\")\n",
    "print(f\"   ðŸ“ Dataset: {DATASET_DIR}\")\n",
    "print(f\"   ðŸ‹ï¸ Pretrained: {PRETRAINED_MODEL_DIR}\")\n",
    "print(f\"   ðŸ’¾ Checkpoints: {CHECKPOINT_DIR}\")\n",
    "print(f\"   ðŸ§  Memory optimized: Batch={BATCH_SIZE}, Patch={PATCH_SIZE}, Accumulation={GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace your current set_random_seed function with this optimized version\n",
    "def set_random_seed(seed=42):\n",
    "    \"\"\"Set random seed with performance optimizations\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Speed optimizations\n",
    "    torch.backends.cudnn.deterministic = False  # Much faster\n",
    "    torch.backends.cudnn.benchmark = True       # Auto-optimize kernels\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True    # RTX 3070 acceleration\n",
    "    torch.backends.cudnn.allow_tf32 = True          # RTX 3070 acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize your ThermalLoss class by pre-registering kernels\n",
    "class ThermalLoss(nn.Module):\n",
    "    def __init__(self, l1_weight=1.0, gradient_weight=0.1, thermal_weight=0.05):\n",
    "        super(ThermalLoss, self).__init__()\n",
    "        self.l1_weight = l1_weight\n",
    "        self.gradient_weight = gradient_weight\n",
    "        self.thermal_weight = thermal_weight\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        \n",
    "        # Pre-register Sobel kernels as buffers (major speed boost)\n",
    "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "        self.register_buffer('sobel_x', sobel_x)\n",
    "        self.register_buffer('sobel_y', sobel_y)\n",
    "        \n",
    "        # Pre-register averaging kernel\n",
    "        kernel = torch.ones(1, 1, 3, 3) / 9.0\n",
    "        self.register_buffer('avg_kernel', kernel)\n",
    "    \n",
    "    def gradient_loss(self, pred, target):\n",
    "        \"\"\"Calculate gradient loss using pre-registered kernels\"\"\"\n",
    "        pred_grad_x = F.conv2d(pred, self.sobel_x, padding=1)\n",
    "        pred_grad_y = F.conv2d(pred, self.sobel_y, padding=1)\n",
    "        target_grad_x = F.conv2d(target, self.sobel_x, padding=1)\n",
    "        target_grad_y = F.conv2d(target, self.sobel_y, padding=1)\n",
    "        \n",
    "        grad_loss = self.l1_loss(pred_grad_x, target_grad_x) + self.l1_loss(pred_grad_y, target_grad_y)\n",
    "        return grad_loss\n",
    "    \n",
    "    def thermal_contrast_loss(self, pred, target):\n",
    "        \"\"\"Loss using pre-registered averaging kernel\"\"\"\n",
    "        pred_mean = F.conv2d(pred, self.avg_kernel, padding=1)\n",
    "        target_mean = F.conv2d(target, self.avg_kernel, padding=1)\n",
    "        \n",
    "        pred_var = F.conv2d((pred - pred_mean)**2, self.avg_kernel, padding=1)\n",
    "        target_var = F.conv2d((target - target_mean)**2, self.avg_kernel, padding=1)\n",
    "        \n",
    "        contrast_loss = self.l1_loss(pred_var, target_var)\n",
    "        return contrast_loss\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        grad = self.gradient_loss(pred, target)\n",
    "        thermal = self.thermal_contrast_loss(pred, target)\n",
    "        \n",
    "        total_loss = (self.l1_weight * l1 + \n",
    "                     self.gradient_weight * grad + \n",
    "                     self.thermal_weight * thermal)\n",
    "        \n",
    "        return total_loss, {'l1': l1.item(), 'gradient': grad.item(), 'thermal': thermal.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d91488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(model, freeze_backbone=True):\n",
    "  \"\"\"Freeze/unfreeze model layers for gradual training\"\"\"\n",
    "  for name, param in model.named_parameters():\n",
    "    if freeze_backbone and not any(layer in name.lower() for layer in ['upsampler', 'lr_conv', 'fea_conv']):\n",
    "      param.requires_grad = False\n",
    "    else:\n",
    "      param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c2212",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psnr(img1, img2, max_val=1.0):\n",
    "  \"\"\"Calculate PSNR between two images\"\"\"\n",
    "  mse = torch.mean((img1 - img2)**2)\n",
    "  if mse == 0:\n",
    "    return float('inf')\n",
    "  return 20 * torch.log10(max_val / torch.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, criterion, device, max_batches=50):\n",
    "  \"\"\"Validate the model on validation set\"\"\"\n",
    "  model.eval()\n",
    "  total_loss = 0\n",
    "  total_psnr = 0\n",
    "  num_batches = 0\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (lr, hr) in enumerate(val_loader):\n",
    "      if batch_idx >= max_batches:\n",
    "        break\n",
    "        \n",
    "      lr, hr = lr.to(device), hr.to(device)\n",
    "      \n",
    "      # Forward pass\n",
    "      with autocast():\n",
    "        sr = model(lr)\n",
    "        loss, loss_components = criterion(sr, hr)\n",
    "      \n",
    "      # Calculate PSNR\n",
    "      psnr = calculate_psnr(sr, hr)\n",
    "      \n",
    "      total_loss += loss.item()\n",
    "      total_psnr += psnr.item()\n",
    "      num_batches += 1\n",
    "  \n",
    "  avg_loss = total_loss / num_batches\n",
    "  avg_psnr = total_psnr / num_batches\n",
    "  \n",
    "  return avg_loss, avg_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, loss, psnr, checkpoint_dir, is_best=False):\n",
    "  \"\"\"Save model checkpoint\"\"\"\n",
    "  checkpoint = {\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    'psnr': psnr,\n",
    "  }\n",
    "  \n",
    "  # Save regular checkpoint\n",
    "  checkpoint_path = os.path.join(checkpoint_dir, f'thermal_epoch_{epoch}.pth')\n",
    "  torch.save(checkpoint, checkpoint_path)\n",
    "  \n",
    "  # Save best model\n",
    "  if is_best:\n",
    "    best_path = os.path.join(checkpoint_dir, 'thermal_best.pth')\n",
    "    torch.save(checkpoint, best_path)\n",
    "    print(f\"ðŸ’« New best model saved! PSNR: {psnr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_info(model, sample_input):\n",
    "  \"\"\"Print model information\"\"\"\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    output = model(sample_input)\n",
    "  \n",
    "  total_params = sum(p.numel() for p in model.parameters())\n",
    "  trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "  \n",
    "  print(f\"ðŸ“Š Model Information:\")\n",
    "  print(f\"   â€¢ Total parameters: {total_params:,}\")\n",
    "  print(f\"   â€¢ Trainable parameters: {trainable_params:,}\")\n",
    "  print(f\"   â€¢ Input shape: {sample_input.shape}\")\n",
    "  print(f\"   â€¢ Output shape: {output.shape}\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc24b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug dataset structure before running training\n",
    "print(\"ðŸ” Debugging dataset paths...\")\n",
    "print(f\"DATASET_DIR: {DATASET_DIR}\")\n",
    "print()\n",
    "\n",
    "# Check expected paths\n",
    "expected_paths = [\n",
    "    os.path.join(DATASET_DIR, 'train', 'HR'),\n",
    "    os.path.join(DATASET_DIR, 'train', f'LR_bicubic', f'X{SCALE}'),\n",
    "    os.path.join(DATASET_DIR, 'val', 'HR'),\n",
    "    os.path.join(DATASET_DIR, 'val', f'LR_bicubic', f'X{SCALE}')\n",
    "]\n",
    "\n",
    "print(\"ðŸ“ Expected directory structure:\")\n",
    "for path in expected_paths:\n",
    "    exists = os.path.exists(path)\n",
    "    if exists:\n",
    "        file_count = len([f for f in os.listdir(path) if f.endswith('.png')])\n",
    "        print(f\"   âœ… {path} - {file_count} files\")\n",
    "    else:\n",
    "        print(f\"   âŒ {path} - NOT FOUND\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Show actual directory structure\n",
    "if os.path.exists(DATASET_DIR):\n",
    "    print(\"ðŸ“‚ Actual directory structure:\")\n",
    "    for root, dirs, files in os.walk(DATASET_DIR):\n",
    "        level = root.replace(DATASET_DIR, '').count(os.sep)\n",
    "        indent = '   ' * level\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")\n",
    "        \n",
    "        # Show image files count\n",
    "        image_files = [f for f in files if f.endswith(('.png', '.jpg'))]\n",
    "        if image_files:\n",
    "            subindent = '   ' * (level + 1)\n",
    "            print(f\"{subindent}({len(image_files)} image files)\")\n",
    "        \n",
    "        if level >= 3:  # Don't go too deep\n",
    "            break\n",
    "else:\n",
    "    print(f\"âŒ Dataset directory doesn't exist: {DATASET_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4595e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug ThermalDataset loading issue\n",
    "print(\"ðŸ”§ Debugging ThermalDataset loading...\")\n",
    "\n",
    "# Test dataset initialization step by step\n",
    "train_hr_dir = os.path.join(DATASET_DIR, 'train', 'HR')\n",
    "train_lr_dir = os.path.join(DATASET_DIR, 'train', f'LR_bicubic', f'X{SCALE}')\n",
    "\n",
    "print(f\"Train HR dir: {train_hr_dir}\")\n",
    "print(f\"Train LR dir: {train_lr_dir}\")\n",
    "print(f\"Directories exist: HR={os.path.exists(train_hr_dir)}, LR={os.path.exists(train_lr_dir)}\")\n",
    "\n",
    "# Test TrainOpt class\n",
    "class TestTrainOpt:\n",
    "    def __init__(self):\n",
    "        self.scale = SCALE\n",
    "        self.phase = 'train'\n",
    "        self.hr_dir = train_hr_dir\n",
    "        self.lr_dir = train_lr_dir\n",
    "        self.ext = '.png'\n",
    "        self.augment = True\n",
    "        self.thermal_augment = True\n",
    "        self.patch_size = PATCH_SIZE\n",
    "        self.n_colors = 1  \n",
    "        self.rgb_range = 1\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.test_every = 1000\n",
    "\n",
    "print(\"ðŸ§ª Testing dataset creation...\")\n",
    "try:\n",
    "    test_opt = TestTrainOpt()\n",
    "    print(f\"   âœ… TestTrainOpt created successfully\")\n",
    "    print(f\"   â€¢ hr_dir: {test_opt.hr_dir}\")\n",
    "    print(f\"   â€¢ lr_dir: {test_opt.lr_dir}\")\n",
    "    print(f\"   â€¢ scale: {test_opt.scale}\")\n",
    "    print(f\"   â€¢ phase: {test_opt.phase}\")\n",
    "    \n",
    "    # Try creating dataset\n",
    "    test_dataset = ThermalDataset(test_opt)\n",
    "    print(f\"   âœ… ThermalDataset created successfully\")\n",
    "    print(f\"   â€¢ Dataset length: {len(test_dataset)}\")\n",
    "    \n",
    "    # Check if dataset has images_hr attribute\n",
    "    if hasattr(test_dataset, 'images_hr'):\n",
    "        print(f\"   â€¢ images_hr length: {len(test_dataset.images_hr)}\")\n",
    "        if test_dataset.images_hr:\n",
    "            print(f\"   â€¢ Sample HR files: {test_dataset.images_hr[:3]}\")\n",
    "    \n",
    "    if hasattr(test_dataset, 'images_lr'):\n",
    "        print(f\"   â€¢ images_lr length: {len(test_dataset.images_lr) if test_dataset.images_lr else 'None'}\")\n",
    "        if test_dataset.images_lr:\n",
    "            print(f\"   â€¢ Sample LR files: {test_dataset.images_lr[:3]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Error creating ThermalDataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_random_seed(SEED)\n",
    "\n",
    "# Setup device\n",
    "if DEVICE == 'auto':\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "\tdevice = torch.device(DEVICE)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "# Check if CUDA setup was successful\n",
    "if device.type == 'cuda':\n",
    "\tprint(f\"[INFO] CUDA device selected: {torch.cuda.get_device_name(device)}\")\n",
    "\tprint(f\"[INFO] CUDA memory available: {torch.cuda.get_device_properties(device).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "\tprint(\"[WARNING] Using CPU - training will be significantly slower\")\n",
    "\n",
    "print(\"[INFO] STARTING OPTIMIZATION PROCESS\")\n",
    "print(f\"[INFO] Scale factor: {SCALE}x\")\n",
    "print(f\"[INFO] Dataset: {DATASET_DIR}\")\n",
    "print(f\"[INFO] Pretrained model: {PRETRAINED_MODEL_DIR}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Setup datasets\n",
    "print(\"[INFO] Loading thermal dataset...\")\n",
    "\n",
    "# Training dataset\n",
    "train_hr_dir = os.path.join(DATASET_DIR, 'train', 'HR')\n",
    "train_lr_dir = os.path.join(DATASET_DIR, 'train', f'LR_bicubic', f'X{SCALE}')\n",
    "\n",
    "# Create options object for ThermalDataset with FIXED repeat calculation\n",
    "class TrainOpt:\n",
    "\tdef __init__(self):\n",
    "\t\tself.scale = SCALE\n",
    "\t\tself.phase = 'train'\n",
    "\t\tself.hr_dir = train_hr_dir\n",
    "\t\tself.lr_dir = train_lr_dir\n",
    "\t\tself.ext = '.png'\n",
    "\t\tself.augment = True\n",
    "\t\tself.thermal_augment = True\n",
    "\t\tself.patch_size = PATCH_SIZE\n",
    "\t\tself.n_colors = 1\n",
    "\t\tself.rgb_range = 1\n",
    "\t\tself.batch_size = BATCH_SIZE\n",
    "\t\tself.test_every = max(1000, len(os.listdir(train_hr_dir)))  # Fix repeat calculation\n",
    "\n",
    "train_dataset = ThermalDataset(TrainOpt())\n",
    "\n",
    "# Validation dataset  \n",
    "val_hr_dir = os.path.join(DATASET_DIR, 'val', 'HR')\n",
    "val_lr_dir = os.path.join(DATASET_DIR, 'val', f'LR_bicubic', f'X{SCALE}')\n",
    "\n",
    "# Create options object for validation dataset\n",
    "class ValOpt:\n",
    "\tdef __init__(self):\n",
    "\t\tself.scale = SCALE\n",
    "\t\tself.phase = 'val'\n",
    "\t\tself.hr_dir = val_hr_dir\n",
    "\t\tself.lr_dir = val_lr_dir\n",
    "\t\tself.ext = '.png'\n",
    "\t\tself.augment = False\n",
    "\t\tself.thermal_augment = False\n",
    "\t\tself.patch_size = PATCH_SIZE\n",
    "\t\tself.n_colors = 1\n",
    "\t\tself.rgb_range = 1\n",
    "\t\tself.batch_size = BATCH_SIZE\n",
    "\t\tself.test_every = 1000\n",
    "\n",
    "val_dataset = ThermalDataset(ValOpt())\n",
    "\n",
    "print(f\"   â€¢ Training samples: {len(train_dataset)}\")\n",
    "print(f\"   â€¢ Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Quick fix if training samples still 0\n",
    "if len(train_dataset) == 0:\n",
    "    print(\"[ERROR] Training dataset length is 0, fixing repeat calculation...\")\n",
    "    # Manually set repeat to 1 for training dataset\n",
    "    train_dataset.repeat = 1\n",
    "    print(f\"   â€¢ Fixed training samples: {len(train_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,    # Keeps workers alive between epochs\n",
    "    prefetch_factor=3,          # More aggressive prefetching\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=2\n",
    ")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Setup model\n",
    "print(\"[INFO] Setting up IMDN model...\")\n",
    "model = IMDN(upscale=SCALE, in_nc=1, out_nc=1)  # Single channel for thermal\n",
    "\n",
    "# Load pretrained weights\n",
    "if os.path.exists(PRETRAINED_MODEL_DIR):\n",
    "\tprint(f\"[INFO] Loading pretrained weights from {PRETRAINED_MODEL_DIR}\")\n",
    "\ttry:\n",
    "\t\tcheckpoint = torch.load(PRETRAINED_MODEL_DIR, map_location='cpu', weights_only=True)\n",
    "\texcept:\n",
    "\t\t# Fallback for older PyTorch versions\n",
    "\t\tcheckpoint = torch.load(PRETRAINED_MODEL_DIR, map_location='cpu')\n",
    "\n",
    "\t# Extract state dict if it's wrapped in a checkpoint\n",
    "\tif isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "\t\tstate_dict = checkpoint['model_state_dict']\n",
    "\telse:\n",
    "\t\tstate_dict = checkpoint\n",
    "\n",
    "\t# Remove 'module.' prefix if present (from DataParallel training)\n",
    "\tif any(key.startswith('module.') for key in state_dict.keys()):\n",
    "\t\tstate_dict = {key.replace('module.', ''): value for key, value in state_dict.items()}\n",
    "\n",
    "\t# Universal weight adaptation for any scale factor\n",
    "\tadapted_state_dict = {}\n",
    "\ttarget_upsampler_channels = SCALE * SCALE  # For thermal: 1 channel * scale^2\n",
    "\t\n",
    "\tfor name, param in state_dict.items():\n",
    "\t\tif name == 'fea_conv.weight' and param.shape[1] == 3:  \n",
    "\t\t\t# RGB input layer -> thermal input layer (3->1 channel)\n",
    "\t\t\tadapted_param = param.mean(dim=1, keepdim=True)\n",
    "\t\t\tadapted_state_dict[name] = adapted_param\n",
    "\t\t\tprint(f\"   â€¢ Adapted {name}: {param.shape} -> {adapted_param.shape}\")\n",
    "\t\t\n",
    "\t\telif name.startswith('upsampler.') and 'weight' in name:\n",
    "\t\t\t# Handle upsampler weight - adapt from RGB to thermal\n",
    "\t\t\tsource_channels = param.shape[0]  # RGB: 3 * source_scale^2\n",
    "\t\t\t\n",
    "\t\t\tif source_channels % 3 == 0:  # Confirm it's RGB (divisible by 3)\n",
    "\t\t\t\tsource_scale_sq = source_channels // 3  # Get source scale^2\n",
    "\t\t\t\t\n",
    "\t\t\t\tif source_scale_sq == target_upsampler_channels:\n",
    "\t\t\t\t\t# Same scale: just average RGB channels to get thermal\n",
    "\t\t\t\t\tparam_reshaped = param.view(3, target_upsampler_channels, param.shape[1], param.shape[2], param.shape[3])\n",
    "\t\t\t\t\tadapted_param = param_reshaped.mean(dim=0)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Different scales: need to adapt the scale\n",
    "\t\t\t\t\tif target_upsampler_channels <= source_scale_sq:\n",
    "\t\t\t\t\t\t# Target scale is smaller: downsample\n",
    "\t\t\t\t\t\tparam_reshaped = param.view(3, source_scale_sq, param.shape[1], param.shape[2], param.shape[3])\n",
    "\t\t\t\t\t\trgb_avg = param_reshaped.mean(dim=0)  # Average RGB channels first\n",
    "\t\t\t\t\t\tadapted_param = rgb_avg[:target_upsampler_channels]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Target scale is larger: upsample by repeating\n",
    "\t\t\t\t\t\tparam_reshaped = param.view(3, source_scale_sq, param.shape[1], param.shape[2], param.shape[3])\n",
    "\t\t\t\t\t\trgb_avg = param_reshaped.mean(dim=0)  # Average RGB channels first\n",
    "\t\t\t\t\t\trepeat_factor = target_upsampler_channels // source_scale_sq\n",
    "\t\t\t\t\t\tremainder = target_upsampler_channels % source_scale_sq\n",
    "\t\t\t\t\t\tadapted_param = rgb_avg.repeat(repeat_factor, 1, 1, 1)\n",
    "\t\t\t\t\t\tif remainder > 0:\n",
    "\t\t\t\t\t\t\tadapted_param = torch.cat([adapted_param, rgb_avg[:remainder]], dim=0)\n",
    "\t\t\t\t\n",
    "\t\t\t\tadapted_state_dict[name] = adapted_param\n",
    "\t\t\t\tprint(f\"   â€¢ Adapted {name}: {param.shape} -> {adapted_param.shape}\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tadapted_state_dict[name] = param\n",
    "\t\t\n",
    "\t\telif name.startswith('upsampler.') and 'bias' in name:\n",
    "\t\t\t# Handle upsampler bias - adapt from RGB to thermal\n",
    "\t\t\tsource_channels = param.shape[0]  # RGB: 3 * source_scale^2\n",
    "\t\t\t\n",
    "\t\t\tif source_channels % 3 == 0:  # Confirm it's RGB (divisible by 3)\n",
    "\t\t\t\tsource_scale_sq = source_channels // 3  # Get source scale^2\n",
    "\t\t\t\t\n",
    "\t\t\t\tif source_scale_sq == target_upsampler_channels:\n",
    "\t\t\t\t\t# Same scale: just average RGB channels to get thermal\n",
    "\t\t\t\t\tparam_reshaped = param.view(3, target_upsampler_channels)\n",
    "\t\t\t\t\tadapted_param = param_reshaped.mean(dim=0)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\t# Different scales: need to adapt the scale\n",
    "\t\t\t\t\tif target_upsampler_channels <= source_scale_sq:\n",
    "\t\t\t\t\t\t# Target scale is smaller: downsample\n",
    "\t\t\t\t\t\tparam_reshaped = param.view(3, source_scale_sq)\n",
    "\t\t\t\t\t\trgb_avg = param_reshaped.mean(dim=0)  # Average RGB channels first\n",
    "\t\t\t\t\t\tadapted_param = rgb_avg[:target_upsampler_channels]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t# Target scale is larger: upsample by repeating\n",
    "\t\t\t\t\t\tparam_reshaped = param.view(3, source_scale_sq)\n",
    "\t\t\t\t\t\trgb_avg = param_reshaped.mean(dim=0)  # Average RGB channels first\n",
    "\t\t\t\t\t\trepeat_factor = target_upsampler_channels // source_scale_sq\n",
    "\t\t\t\t\t\tremainder = target_upsampler_channels % source_scale_sq\n",
    "\t\t\t\t\t\tadapted_param = rgb_avg.repeat(repeat_factor)\n",
    "\t\t\t\t\t\tif remainder > 0:\n",
    "\t\t\t\t\t\t\tadapted_param = torch.cat([adapted_param, rgb_avg[:remainder]], dim=0)\n",
    "\t\t\t\t\n",
    "\t\t\t\tadapted_state_dict[name] = adapted_param\n",
    "\t\t\t\tprint(f\"   â€¢ Adapted {name}: {param.shape} -> {adapted_param.shape}\")\n",
    "\t\t\telse:\n",
    "\t\t\t\tadapted_state_dict[name] = param\n",
    "\t\t\n",
    "\t\telse:\n",
    "\t\t\t# All other layers: keep as is\n",
    "\t\t\tadapted_state_dict[name] = param\n",
    "\n",
    "\t# Load adapted weights\n",
    "\tmissing_keys, unexpected_keys = model.load_state_dict(adapted_state_dict, strict=False)\n",
    "\tif missing_keys:\n",
    "\t\tprint(f\"[ERROR] Missing keys: {missing_keys}\")\n",
    "\tif unexpected_keys:\n",
    "\t\tprint(f\"[ERROR] Unexpected keys: {unexpected_keys}\")\n",
    "\t\n",
    "\tprint(f\"[INFO] Successfully adapted pretrained model from RGB to thermal with {SCALE}x scaling\")\n",
    "else:\n",
    "\tprint(f\"[ERROR] Pretrained model not found at {PRETRAINED_MODEL_DIR}\")\n",
    "\tprint(\"[ERROR] Training from scratch (this will take much longer)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = model.to(device)\n",
    "model = model.to(memory_format=torch.channels_last)  # Memory layout optimization\n",
    "\n",
    "# Print model info\n",
    "sample_input = torch.randn(1, 1, 64, 64).to(device)\n",
    "print_model_info(model, sample_input)\n",
    "\n",
    "# Setup loss function\n",
    "criterion = ThermalLoss(\n",
    "\tl1_weight=L1_WEIGHT,\n",
    "\tgradient_weight=GRADIENT_WEIGHT,\n",
    "\tthermal_weight=THERMAL_WEIGHT\n",
    ").to(device)\n",
    "\n",
    "# Setup optimizer\n",
    "optimizer = optim.AdamW(\n",
    "\tmodel.parameters(),\n",
    "\tlr=LR,\n",
    "\tweight_decay=WEIGHT_DECAY,\n",
    "\tbetas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "\toptimizer,\n",
    "\tT_max=EPOCHS,\n",
    "\teta_min=LR * 0.01\n",
    ")\n",
    "\n",
    "# Setup mixed precision training\n",
    "scaler = GradScaler() if MIXED_PRECISION and device.type == 'cuda' else None\n",
    "\n",
    "# Gradual unfreezing setup\n",
    "if GRADUAL_UNFREEZE:\n",
    "\tprint(\"[INFO] Starting with frozen backbone (gradual unfreezing enabled)\")\n",
    "\tfreeze_layers(model, freeze_backbone=True)\n",
    "\n",
    "# Memory optimization\n",
    "print(\"[INFO] Memory optimization enabled:\")\n",
    "print(f\"   â€¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   â€¢ Patch size: {PATCH_SIZE}\")\n",
    "print(f\"   â€¢ Gradient accumulation: {GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"   â€¢ Mixed precision: {MIXED_PRECISION}\")\n",
    "\n",
    "# Training loop with gradient accumulation\n",
    "print(\"[INFO] Starting training...\")\n",
    "print()\n",
    "\n",
    "best_psnr = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\t# Gradual unfreezing\n",
    "\tif GRADUAL_UNFREEZE and epoch == FREEZE_EPOCHS:\n",
    "\t\tprint(\"[INFO] Unfreezing backbone layers\")\n",
    "\t\tfreeze_layers(model, freeze_backbone=False)\n",
    "\t\t# Reduce learning rate when unfreezing\n",
    "\t\tfor param_group in optimizer.param_groups:\n",
    "\t\t\tparam_group['lr'] *= 0.5\n",
    "\n",
    "\tmodel.train()\n",
    "\tepoch_loss = 0\n",
    "\tepoch_l1 = 0\n",
    "\tepoch_gradient = 0\n",
    "\tepoch_thermal = 0\n",
    "\t\n",
    "\t# Gradient accumulation setup\n",
    "\toptimizer.zero_grad()\n",
    "\n",
    "\tfor batch_idx, (lr, hr) in enumerate(train_loader):\n",
    "\t\tlr, hr = lr.to(device), hr.to(device)\n",
    "\t\t\n",
    "\t\t# Forward pass with mixed precision\n",
    "\t\tif scaler is not None:\n",
    "\t\t\twith autocast():\n",
    "\t\t\t\tsr = model(lr)\n",
    "\t\t\t\tloss, loss_components = criterion(sr, hr)\n",
    "\t\t\t\tloss = loss / GRADIENT_ACCUMULATION_STEPS  # Scale loss for accumulation\n",
    "\t\t\n",
    "\t\t\t# Backward pass\n",
    "\t\t\tscaler.scale(loss).backward()\n",
    "\t\telse:\n",
    "\t\t\tsr = model(lr)\n",
    "\t\t\tloss, loss_components = criterion(sr, hr)\n",
    "\t\t\tloss = loss / GRADIENT_ACCUMULATION_STEPS  # Scale loss for accumulation\n",
    "\t\t\tloss.backward()\n",
    "\t\t\n",
    "\t\t# Accumulate losses (scale back for logging)\n",
    "\t\tepoch_loss += loss.item() * GRADIENT_ACCUMULATION_STEPS\n",
    "\t\tepoch_l1 += loss_components['l1']\n",
    "\t\tepoch_gradient += loss_components['gradient']\n",
    "\t\tepoch_thermal += loss_components['thermal']\n",
    "\t\t\n",
    "\t\t# Update weights every GRADIENT_ACCUMULATION_STEPS\n",
    "\t\tif (batch_idx + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "\t\t\tif scaler is not None:\n",
    "\t\t\t\tscaler.step(optimizer)\n",
    "\t\t\t\tscaler.update()\n",
    "\t\t\telse:\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()Updates for week [Sept. 8] - [Sept. 12]\n",
    "\n",
    "\n",
    "Work Completed & Current Status\n",
    "\t\t\t\n",
    "\t\t\t# Clear cache periodically\n",
    "\t\t\tif batch_idx % 20 == 0:\n",
    "\t\t\t\ttorch.cuda.empty_cache()\n",
    "\t\t\n",
    "\t\t# Logging\n",
    "\t\tif batch_idx % LOG_INTERVAL == 0:\n",
    "\t\t\tprogress = 100.0 * batch_idx / len(train_loader)\n",
    "\t\t\tcurrent_lr = optimizer.param_groups[0]['lr']\n",
    "\t\t\tprint(f\"Epoch {epoch:3d} [{batch_idx:4d}/{len(train_loader)} ({progress:5.1f}%)] \"\n",
    "\t\t\t\t  f\"Loss: {loss.item() * GRADIENT_ACCUMULATION_STEPS:.6f} L1: {loss_components['l1']:.6f} \"\n",
    "\t\t\t\t  f\"Grad: {loss_components['gradient']:.6f} Thermal: {loss_components['thermal']:.6f} \"\n",
    "\t\t\t\t  f\"LR: {current_lr:.2e}\")\n",
    "\n",
    "\t# Update learning rate\n",
    "\tscheduler.step()\n",
    "\n",
    "\t# Calculate epoch averages\n",
    "\tavg_loss = epoch_loss / len(train_loader)\n",
    "\tavg_l1 = epoch_l1 / len(train_loader)\n",
    "\tavg_gradient = epoch_gradient / len(train_loader)\n",
    "\tavg_thermal = epoch_thermal / len(train_loader)\n",
    "\n",
    "\t# Validation\n",
    "\tval_loss, val_psnr = 0, 0\n",
    "\tif epoch % VAL_INTERVAL == 0:\n",
    "\t\tval_loss, val_psnr = validate_model(model, val_loader, criterion, device)\n",
    "\t\t\n",
    "\t\t# Save checkpoint if best\n",
    "\t\tis_best = val_psnr > best_psnr\n",
    "\t\tif is_best:\n",
    "\t\t\tbest_psnr = val_psnr\n",
    "\t\t\n",
    "\t\tsave_checkpoint(model, optimizer, epoch, val_loss, val_psnr, CHECKPOINT_DIR, is_best)\n",
    "\n",
    "\t# Epoch summary\n",
    "\telapsed = time.time() - start_time\n",
    "\tprint(f\"Epoch {epoch:3d} Summary: Loss={avg_loss:.6f} (L1:{avg_l1:.4f}, Grad:{avg_gradient:.4f}, Thermal:{avg_thermal:.4f}) \"\n",
    "\t\t  f\"Val_PSNR={val_psnr:.2f}dB Best={best_psnr:.2f}dB Time={elapsed/60:.1f}min\")\n",
    "\tprint(\"-\" * 100)\n",
    "\n",
    "# Final summary\n",
    "total_time = time.time() - start_time\n",
    "print()\n",
    "print(\"[INFO] Training completed!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"[INFO] Best validation PSNR: {best_psnr:.2f} dB\")\n",
    "print(f\"[INFO] Total training time: {total_time/3600:.2f} hours\")\n",
    "print(f\"[INFO] Best model saved at: {os.path.join(CHECKPOINT_DIR, 'thermal_best.pth')}\")\n",
    "print()\n",
    "print(\"[INFO] Your thermal super-resolution model is ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
